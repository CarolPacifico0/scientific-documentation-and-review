The idea to create this repository came from my experience working across different types of projects. 
Although each project is unique, they often share common patterns: clear strengths, recurring areas for improvement, approaches that worked well, and gaps that need to be addressed to keep the work sustainable over time.

There is a growing focus on using AI to make tasks faster and more automated. However, without a solid understanding of the problem and the underlying scientific principles, AI becomes little more than a shot in the dark - often leading to misleading results rather than real insights.

For this reason, I believe scientific criteria must remain central to any analytical workflow: critical evaluation, interpretability, reproducibility, and the ability to clearly explain what is being done and why.

This repository shares documentation and review practices that have worked well for me across scientific and data-driven projects. I hope these examples can help others structure, evaluate, and communicate their analyses more effectively.
