The idea to create this repository came from my experience working across different types of projects. 
Although each project is unique, they often share common patterns: clear strengths, recurring areas for improvement, approaches that worked well, and gaps that need to be addressed to keep the work sustainable over time.

There is a growing focus on using AI to make tasks faster and more automated. However, without a solid understanding of the problem and the underlying scientific principles, AI becomes little more than a shot in the dark - often leading to misleading results rather than real insights.

For this reason, I believe scientific criteria must remain central to any analytical workflow: critical evaluation, interpretability, reproducibility, and the ability to clearly explain what is being done and why.

This repository shares documentation and review practices that have worked well for me across scientific and data-driven projects. I hope these examples can help others structure, evaluate, and communicate their analyses more effectively.

Ideally, start with a clear central question or objective (some examples are shared below). 
Once it is defined, stay focused on it throughout the analysis. 

It can be very tempting to start exploring additional patterns or questions along the way, but this often leads to a common pitfall: by the end of the analysis, you may have answered many interesting questions - except the one you originally set out to address. Donâ€™t fall for it!! 

## Overview

This repository includes three examples of scientific analysis documentation and technical review:

- Gene expression analysis;  
- Data curation and quality control (QC) of biological data; 
- AI-generated analyses and their critical review.  

**Note:** No sensitive data are shared in this repository. All examples are illustrative and do not include data analyzed as part of professional work or collaborations.
